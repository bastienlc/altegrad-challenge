\documentclass[switch, 12pt]{article}

\usepackage{preprint}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage[numbers,square]{natbib}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage[colorlinks = true,
    linkcolor = purple,
    urlcolor  = blue,
    citecolor = cyan,
    anchorcolor = black]{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lineno}
\usepackage{float}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}
\usepackage{float}
\usepackage{subfloat}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{authblk}

\DeclareMathOperator{\leakyrelu}{LeakyReLU}

\bibliographystyle{unsrtnat}
\setlist[enumerate,1]{leftmargin=2em}
\titlespacing\section{0pt}{12pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}
\titlespacing\subsection{0pt}{10pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}
\titlespacing\subsubsection{0pt}{8pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}
\renewcommand*{\Authfont}{\bfseries}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\minimize}{minimize}

\title{Molecule Retrieval with Natural Language Queries}
\author[1]{Sofiane Ezzehi}
\author[1]{Bastien Le Chenadec}
\affil[1]{Ã‰cole des Ponts ParisTech}

\begin{document}

\maketitle

\begin{contribstatement}

\end{contribstatement}
\vspace{0.35cm}

\begin{multicols}{2}
    \section{Introduction}

    The goal of this challenge is to retrieve molecules from a database using natural language queries. Each sample in the dataset is constituted of a ChEBI description of a molecule, which is a text describing its structure and properties, and an undirected graph representing the molecule with embeddings for each node. The embeddings are pre-computed using the Mol2Vec algorithm \cite{mol2vec}. Given a textual query, the goal is to retrieve the molecule that best matches the query. The evaluation metric is the label ranking average precision score (LRAP) which is equivalent to the mean reciprocal rank (MRR) in our case.

    The challenging part of this task is to find a way to combine two very different modalities : texts and graphs. One way to achieve this is to use contrastive learning : one model encodes the text and the other encodes the graph. The two encoders are then trained to project similar samples close to each other in the embedding space. This approach has been shown to be effective in many tasks \cite{chen-2020,gao-2021}.

    \section{Data}

    \section{Method}

    In this section, we describe the different models we used to encode the text and the graph. The text encoder and graph encoders are two separate models that are trained jointly using contrastive learning, so that they share the same embedding space.

    \subsection{Graph Attention Networks}

    Graph Attention Networks (GAT) \cite{velickovic-2018} have been shown to be effective in many tasks. Like other graph neural networks, GATs aggregate information from the neighbors of each node to compute its embedding. The main difference with other models is that GATs use an attention mechanism to weight the neighbors of each node. Specifically we used the improved version of GATs suggested in \cite{brody-2021}.

    Let $G$ be an undirected graph with $N$ nodes denoted $\llbracket1, N\rrbracket$. Let $d$ be the dimension of the node embeddings, and $h_1,\dots,h_N\in\R^d$ be the said embeddings. Let $W\in\R^{d'\times d}$ and $a\in\R^{2d'}$. The attention weights are :
    \begin{equation}
        e(h_i,h_j) = a^T \leakyrelu([Wh_i || Wh_j])
    \end{equation}
    where $||$ denotes the concatenation operator. The attention weights are normalized using the softmax operator :
    \begin{equation}
        \alpha_{ij} = \frac{\exp(e(h_i,h_j))}{\sum_{k\in\mathcal{N}_i}\exp(e(h_i,h_k))}
    \end{equation}
    where $\mathcal{N}_i$ denotes the set of neighbors of node $i$ in $G$. This mechanism clearly allows the batch processing of graphs with different sizes. The embedding of node $i$ is then computed as :
    \begin{equation}
        h_i' = \leakyrelu\left(\sum_{j\in\mathcal{N}_i}\alpha_{ij}Wh_j\right)
    \end{equation}
    In general we will use multi-head attention, with $K$ heads, $a^{(1)},\dots,a^{(K)}\in\R^{2d'/K}$ and $W^{(1)},\dots,W^{(K)}\in\R^{d'/K\times d}$ :
    \begin{equation}
        h_i' = \leakyrelu\left(\frac{1}{K}\sum_{k=1}^K\sum_{j\in\mathcal{N}_i}\alpha_{ij}^{(k)}W^{(k)}h_j\right)
    \end{equation}
    Furthermore, we will stack multiple GAT layers to obtain a deeper model. We may also apply a multi-layer perceptron to the embeddings of the last layer to obtain a more expressive representation.


    \subsection{DiffPool}

    \cite{ying-2018}

    \subsection{Language modelling}

    We used a pretrained large language model (LLM) to encode the text. Specifically, we settled on \texttt{sentence-transformers/all-MiniLM-L6-v2} which is a distilled version of MiniLM \cite{wang-2020} known for its efficiency and relatively small size. The sentence embeddings are obtained by averaging the embeddings of the tokens in the sentence, which yields a 384-dimensional vector.

    \section{Training}

    \subsection{Loss}

    \subsection{Training procedure}

    \section{Results}

    \newpage

    \bibliography{bibliography}

\end{multicols}

\end{document}