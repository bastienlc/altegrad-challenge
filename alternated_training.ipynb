{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import time\n",
    "from typing import Union\n",
    "from utils import contrastive_loss\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader as GeometricDataLoader\n",
    "from load import load_dataset\n",
    "from models.gat import GATEncoder\n",
    "from models.baseline import TextEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings_dim = 384\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_loader, val_loader = load_dataset(tokenizer)\n",
    "\n",
    "graph_encoder = GATEncoder(\n",
    "    num_node_features=300,\n",
    "    nout=embeddings_dim,\n",
    "    mlp_hid=1000,\n",
    "    att_hidden_dim=600,\n",
    "    att_out_dim=1000,\n",
    "    nheads=20,\n",
    "    dropout=0.1,\n",
    "    alpha=0.02,\n",
    "    attention_depth=1,\n",
    ").to(device)\n",
    "\n",
    "text_encoder = TextEncoder(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(\n",
    "    graph_encoder: nn.Module,\n",
    "    text_encoder: nn.Module,\n",
    "    loader: GeometricDataLoader,\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    graph_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    graph_embeddings = []\n",
    "    text_embeddings = []\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch.input_ids\n",
    "            batch.pop(\"input_ids\")\n",
    "            attention_mask = batch.attention_mask\n",
    "            batch.pop(\"attention_mask\")\n",
    "            graph_batch = batch\n",
    "\n",
    "            x_graph = graph_encoder(graph_batch.to(device))\n",
    "            x_text = text_encoder(input_ids.to(device), attention_mask.to(device))\n",
    "\n",
    "            loss += contrastive_loss(x_graph, x_text).item()\n",
    "\n",
    "            for output in x_graph:\n",
    "                graph_embeddings.append(output.tolist())\n",
    "            for output in x_text:\n",
    "                text_embeddings.append(output.tolist())\n",
    "\n",
    "    similarity = cosine_similarity(text_embeddings, graph_embeddings)\n",
    "\n",
    "    return (\n",
    "        loss / len(loader),\n",
    "        label_ranking_average_precision_score(np.eye(len(similarity)), similarity),\n",
    "    )\n",
    "\n",
    "\n",
    "def train(\n",
    "    graph_encoder: nn.Module,\n",
    "    text_encoder: nn.Module,\n",
    "    graph_optimizer: optim.Optimizer,\n",
    "    text_optimizer: optim.Optimizer,\n",
    "    train_loader: GeometricDataLoader,\n",
    "    val_loader: GeometricDataLoader,\n",
    "    nb_epochs: int = 5,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    load_from: Union[str, None] = None,\n",
    "    save_name: str = \"model\",\n",
    "):\n",
    "    writer = SummaryWriter()\n",
    "    epoch = 0\n",
    "    loss = 0\n",
    "    loss_averager = 0\n",
    "    losses = []\n",
    "    time1 = time.time()\n",
    "    print_every = 50\n",
    "    best_validation_loss = 1e100\n",
    "    best_validation_score = 0\n",
    "\n",
    "    if load_from is not None:\n",
    "        checkpoint = torch.load(load_from)\n",
    "        graph_encoder.load_state_dict(checkpoint[\"graph_encoder_state_dict\"])\n",
    "        text_encoder.load_state_dict(checkpoint[\"text_encoder_state_dict\"])\n",
    "        graph_optimizer.load_state_dict(checkpoint[\"graph_optimizer_state_dict\"])\n",
    "        text_optimizer.load_state_dict(checkpoint[\"text_optimizer_state_dict\"])\n",
    "        best_validation_loss = checkpoint[\"val_loss\"]\n",
    "        best_validation_score = checkpoint[\"val_score\"]\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        print(\n",
    "            \"Loaded model from {}, best_validation_score={}, best validation loss={}\".format(\n",
    "                load_from, best_validation_score, best_validation_loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for e in range(epoch + 1, nb_epochs):\n",
    "        print(\"--------------------EPOCH {}--------------------\".format(e))\n",
    "        graph_encoder.train()\n",
    "        # Train text encoder 1/2 of the time and not the first epoch\n",
    "        if e % 2 == 1:\n",
    "            print(\"Training only graph encoder\")\n",
    "            text_encoder.eval()\n",
    "            graph_encoder.train()\n",
    "            train_text = False\n",
    "        else:\n",
    "            print(\"Training only text encoder\")\n",
    "            text_encoder.train()\n",
    "            graph_encoder.eval()\n",
    "            train_text = True\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch.input_ids\n",
    "            batch.pop(\"input_ids\")\n",
    "            attention_mask = batch.attention_mask\n",
    "            batch.pop(\"attention_mask\")\n",
    "            graph_batch = batch\n",
    "\n",
    "            if train_text:\n",
    "                with torch.no_grad():\n",
    "                    x_graph = graph_encoder(graph_batch.to(device))\n",
    "                x_text = text_encoder(input_ids.to(device), attention_mask.to(device))\n",
    "                current_loss = contrastive_loss(x_graph, x_text)\n",
    "                text_optimizer.zero_grad()\n",
    "                current_loss.backward()\n",
    "                text_optimizer.step()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    x_text = text_encoder(\n",
    "                        input_ids.to(device), attention_mask.to(device)\n",
    "                    )\n",
    "                x_graph = graph_encoder(graph_batch.to(device))\n",
    "                current_loss = contrastive_loss(x_graph, x_text)\n",
    "                graph_optimizer.zero_grad()\n",
    "                current_loss.backward()\n",
    "                graph_optimizer.step()\n",
    "\n",
    "            loss += current_loss.item()\n",
    "            loss_averager += 1\n",
    "\n",
    "            if batch_idx % print_every == 0 and batch_idx > 0:\n",
    "                loss /= loss_averager\n",
    "                time2 = time.time()\n",
    "                print(\n",
    "                    \"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(\n",
    "                        batch_idx, time2 - time1, loss\n",
    "                    )\n",
    "                )\n",
    "                losses.append(loss)\n",
    "                writer.add_scalar(\"Loss/train\", loss, e * len(train_loader) + batch_idx)\n",
    "                loss = 0\n",
    "                loss_averager = 0\n",
    "\n",
    "        step = (e + 1) * len(train_loader)\n",
    "\n",
    "        print(\n",
    "            \"Computing metrics on validation set... (time={:.4f}s)\".format(\n",
    "                time.time() - time1\n",
    "            )\n",
    "        )\n",
    "        val_loss, val_score = get_metrics(\n",
    "            graph_encoder, text_encoder, val_loader, device=device\n",
    "        )\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, step)\n",
    "        writer.add_scalar(\"Score/val\", val_score, step)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        print(\n",
    "            \"Epoch \" + str(e) + \" finished with val_loss \" + str(val_loss),\n",
    "            \"and val_score\",\n",
    "            val_score,\n",
    "        )\n",
    "\n",
    "        best_validation_loss = min(best_validation_loss, val_loss)\n",
    "        best_validation_score = max(best_validation_score, val_score)\n",
    "\n",
    "        if best_validation_loss == val_loss or best_validation_score == val_score:\n",
    "            print(\"Saving checkpoint... \", end=\"\")\n",
    "            save_path = os.path.join(\"./outputs/\", save_name + str(e) + \".pt\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": e,\n",
    "                    \"graph_encoder_state_dict\": graph_encoder.state_dict(),\n",
    "                    \"text_encoder_state_dict\": text_encoder.state_dict(),\n",
    "                    \"graph_optimizer_state_dict\": graph_optimizer.state_dict(),\n",
    "                    \"text_optimizer_state_dict\": text_optimizer.state_dict(),\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_score\": val_score,\n",
    "                },\n",
    "                save_path,\n",
    "            )\n",
    "            print(\"done : {}\".format(save_path))\n",
    "\n",
    "    writer.close()\n",
    "    return save_path, best_validation_loss, best_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_optimizer = optim.AdamW(\n",
    "    graph_encoder.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=0.01\n",
    ")\n",
    "\n",
    "text_optimizer = optim.AdamW(\n",
    "    text_encoder.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=0.01\n",
    ")\n",
    "\n",
    "save_path, _, _ = train(\n",
    "    graph_encoder,\n",
    "    text_encoder,\n",
    "    graph_optimizer,\n",
    "    text_optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    nb_epochs=50,\n",
    "    device=device,\n",
    "    save_name=\"alternated\",\n",
    "    load_from=\"./outputs/alternated_fix_30.pt\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
