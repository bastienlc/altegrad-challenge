{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One sided learning\n",
    "\n",
    "In some cases we may want to train only the graph encoder with the initial text embeddings, because it may require a lot of iterations to get a good representation of the graphs. This way it will train much faster than if the text encoder is trained simultaneously. Obviously the overall performances will be lower, but we can then fine-tune the whole model. Hopefully this will allow us to get a better graph encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "import torch.nn as nn\n",
    "from utils import train\n",
    "from models.baseline import TextEncoder\n",
    "from transform import OneSidedDataset\n",
    "from models.diffpool import DiffPoolEncoder, DiffPoolModel\n",
    "from metrics import Metrics\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train only the graph encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings_dim = 384\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text_encoder = TextEncoder(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataset with the text embeddings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    text_encoder: nn.Module,\n",
    "    device: torch.device,\n",
    "    batch_size: int = 32,\n",
    "    root=\".\",\n",
    "    features=[],\n",
    "    shuffle=True,\n",
    "):\n",
    "    gt = np.load(f\"{root}/data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "    train_dataset = OneSidedDataset(\n",
    "        root=f\"{root}/data/\",\n",
    "        gt=gt,\n",
    "        split=\"train\",\n",
    "        tokenizer=tokenizer,\n",
    "        text_encoder=text_encoder,\n",
    "        device=device,\n",
    "        features=features,\n",
    "    )\n",
    "    val_dataset = OneSidedDataset(\n",
    "        root=f\"{root}/data/\",\n",
    "        gt=gt,\n",
    "        split=\"val\",\n",
    "        tokenizer=tokenizer,\n",
    "        text_encoder=text_encoder,\n",
    "        device=device,\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneSidedModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_encoder,\n",
    "    ):\n",
    "        super(OneSidedModel, self).__init__()\n",
    "        self.graph_encoder = graph_encoder\n",
    "\n",
    "    def forward(self, graph_batch, input_ids, attention_mask):\n",
    "        graph_encoded = self.graph_encoder(graph_batch)\n",
    "        return graph_encoded, graph_batch.y\n",
    "\n",
    "    def get_text_encoder(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_graph_encoder(self):\n",
    "        return self.graph_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_encoder = DiffPoolEncoder(\n",
    "    300,\n",
    "    embeddings_dim,\n",
    "    d_pooling_layers=[15, 5, 1],\n",
    "    d_encoder_hidden_dims=[600, 600, 600],\n",
    "    d_encoder_linear_layers=[[300], [300], [300]],\n",
    "    d_encoder_num_heads=[3, 3, 3],\n",
    "    d_encoder_num_layers=[3, 3, 2],\n",
    "    d_linear=1000,\n",
    "    dropout=0.05,\n",
    ").to(device)\n",
    "\n",
    "model = OneSidedModel(graph_encoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_dataset(tokenizer, text_encoder, device, batch_size=256)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=0.01\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiplicativeLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda epoch: 0.97,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "save_path, _, _ = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    scheduler=scheduler,\n",
    "    nb_epochs=300,\n",
    "    device=device,\n",
    "    print_every=20,\n",
    "    load_optimizer=True,\n",
    "    metrics=Metrics(loss=\"circle\"),\n",
    "    save_name=\"one_side_circle_256_\",\n",
    "    load_from=\"./outputs/one_side_circle_256_15.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish training the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = \"./outputs/one_side_circle_256_32.pt\"\n",
    "save_path = \"./outputs/one_side_circle_256_full_32.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(load_from)\n",
    "\n",
    "full_model = DiffPoolModel(\n",
    "    model_name=model_name,\n",
    "    num_node_features=300,\n",
    "    nout=embeddings_dim,\n",
    ").to(device)\n",
    "\n",
    "# change keys of checkpoint to remove 'graph_encoder.' prefix\n",
    "new_state_dict = {}\n",
    "for k, v in checkpoint[\"model_state_dict\"].items():\n",
    "    if k.startswith(\"graph_encoder.\"):\n",
    "        name = k[14:]  # remove 'graph_encoder.' prefix\n",
    "        new_state_dict[name] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "full_model.graph_encoder.load_state_dict(new_state_dict)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    full_model.parameters(), lr=1e-6, betas=(0.9, 0.999), weight_decay=0.01\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiplicativeLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda epoch: 0.97,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# save this checkpoint\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": checkpoint[\"epoch\"],\n",
    "        \"model_state_dict\": full_model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"val_loss\": checkpoint[\"val_loss\"],\n",
    "        \"val_score\": checkpoint[\"val_score\"],\n",
    "        \"time\": checkpoint[\"time\"],\n",
    "    },\n",
    "    save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+-----------------------------------+----------------+------------+\n",
      "| Layer                                          | Input Shape                       | Output Shape   | #Param     |\n",
      "|------------------------------------------------+-----------------------------------+----------------+------------|\n",
      "| DiffPoolEncoder                                | [2076, 2076]                      | [64, 384]      | 11,454,805 |\n",
      "| ├─(pooling_layers)ModuleList                   | --                                | --             | 5,252,421  |\n",
      "| │    └─(0)GATEncoder                           | [2076, 300], [2076, 2076], [2076] | [2076, 15]     | 1,994,415  |\n",
      "| │    │    └─(gat)GAT                           | [2076, 300], [2, 4318]            | [2076, 600]    | 1,809,600  |\n",
      "| │    │    │    └─(dropout)Dropout              | [2076, 600]                       | [2076, 600]    | --         |\n",
      "| │    │    │    └─(act)ReLU                     | [2076, 600]                       | [2076, 600]    | --         |\n",
      "| │    │    │    └─(convs)ModuleList             | --                                | --             | 1,807,200  |\n",
      "| │    │    │    │    └─(0)GATv2Conv             | [2076, 300], [2, 4318]            | [2076, 600]    | 362,400    |\n",
      "| │    │    │    │    └─(1)GATv2Conv             | [2076, 600], [2, 4318]            | [2076, 600]    | 722,400    |\n",
      "| │    │    │    │    └─(2)GATv2Conv             | [2076, 600], [2, 4318]            | [2076, 600]    | 722,400    |\n",
      "| │    │    │    └─(norms)ModuleList             | --                                | --             | 2,400      |\n",
      "| │    │    │    │    └─(0)BatchNorm             | [2076, 600]                       | [2076, 600]    | 1,200      |\n",
      "| │    │    │    │    │    └─(module)BatchNorm1d | [2076, 600]                       | [2076, 600]    | 1,200      |\n",
      "| │    │    │    │    └─(1)BatchNorm             | [2076, 600]                       | [2076, 600]    | 1,200      |\n",
      "| │    │    │    │    │    └─(module)BatchNorm1d | [2076, 600]                       | [2076, 600]    | 1,200      |\n",
      "| │    │    │    │    └─(2)Identity              | --                                | --             | --         |\n",
      "| │    │    └─(linear_layers)ModuleList          | --                                | --             | 184,815    |\n",
      "| │    │    │    └─(0)Linear                     | [2076, 600]                       | [2076, 300]    | 180,300    |\n",
      "| │    │    │    └─(1)Linear                     | [2076, 300]                       | [2076, 15]     | 4,515      |\n",
      "| │    │    └─(activation)ReLU                   | [2076, 300]                       | [2076, 300]    | --         |\n",
      "| │    └─(1)GATEncoder                           | [960, 300], [960, 960], [960]     | [960, 5]       | 1,991,405  |\n",
      "| │    │    └─(gat)GAT                           | [960, 300], [2, 14400]            | [960, 600]     | 1,809,600  |\n",
      "| │    │    │    └─(dropout)Dropout              | [960, 600]                        | [960, 600]     | --         |\n",
      "| │    │    │    └─(act)ReLU                     | [960, 600]                        | [960, 600]     | --         |\n",
      "| │    │    │    └─(convs)ModuleList             | --                                | --             | 1,807,200  |\n",
      "| │    │    │    │    └─(0)GATv2Conv             | [960, 300], [2, 14400]            | [960, 600]     | 362,400    |\n",
      "| │    │    │    │    └─(1)GATv2Conv             | [960, 600], [2, 14400]            | [960, 600]     | 722,400    |\n",
      "| │    │    │    │    └─(2)GATv2Conv             | [960, 600], [2, 14400]            | [960, 600]     | 722,400    |\n",
      "| │    │    │    └─(norms)ModuleList             | --                                | --             | 2,400      |\n",
      "| │    │    │    │    └─(0)BatchNorm             | [960, 600]                        | [960, 600]     | 1,200      |\n",
      "| │    │    │    │    │    └─(module)BatchNorm1d | [960, 600]                        | [960, 600]     | 1,200      |\n",
      "| │    │    │    │    └─(1)BatchNorm             | [960, 600]                        | [960, 600]     | 1,200      |\n",
      "| │    │    │    │    │    └─(module)BatchNorm1d | [960, 600]                        | [960, 600]     | 1,200      |\n",
      "| │    │    │    │    └─(2)Identity              | --                                | --             | --         |\n",
      "| │    │    └─(linear_layers)ModuleList          | --                                | --             | 181,805    |\n",
      "| │    │    │    └─(0)Linear                     | [960, 600]                        | [960, 300]     | 180,300    |\n",
      "| │    │    │    └─(1)Linear                     | [960, 300]                        | [960, 5]       | 1,505      |\n",
      "| │    │    └─(activation)ReLU                   | [960, 300]                        | [960, 300]     | --         |\n",
      "| │    └─(2)GATEncoder                           | [320, 300], [320, 320], [320]     | [320, 1]       | 1,266,601  |\n",
      "| │    │    └─(gat)GAT                           | [320, 300], [2, 1600]             | [320, 600]     | 1,086,000  |\n",
      "| │    │    │    └─(dropout)Dropout              | [320, 600]                        | [320, 600]     | --         |\n",
      "| │    │    │    └─(act)ReLU                     | [320, 600]                        | [320, 600]     | --         |\n",
      "| │    │    │    └─(convs)ModuleList             | --                                | --             | 1,084,800  |\n",
      "| │    │    │    │    └─(0)GATv2Conv             | [320, 300], [2, 1600]             | [320, 600]     | 362,400    |\n",
      "| │    │    │    │    └─(1)GATv2Conv             | [320, 600], [2, 1600]             | [320, 600]     | 722,400    |\n",
      "| │    │    │    └─(norms)ModuleList             | --                                | --             | 1,200      |\n",
      "| │    │    │    │    └─(0)BatchNorm             | [320, 600]                        | [320, 600]     | 1,200      |\n",
      "| │    │    │    │    │    └─(module)BatchNorm1d | [320, 600]                        | [320, 600]     | 1,200      |\n",
      "| │    │    │    │    └─(1)Identity              | --                                | --             | --         |\n",
      "| │    │    └─(linear_layers)ModuleList          | --                                | --             | 180,601    |\n",
      "| │    │    │    └─(0)Linear                     | [320, 600]                        | [320, 300]     | 180,300    |\n",
      "| │    │    │    └─(1)Linear                     | [320, 300]                        | [320, 1]       | 301        |\n",
      "| │    │    └─(activation)ReLU                   | [320, 300]                        | [320, 300]     | --         |\n",
      "| ├─(embedding_layers)ModuleList                 | --                                | --             | 5,517,000  |\n",
      "| │    └─(0)GATEncoder                           | [2076, 300], [2076, 2076], [2076] | [2076, 300]    | 2,080,200  |\n",
      "| │    │    └─(gat)GAT                           | [2076, 300], [2, 4318]            | [2076, 600]    | 1,809,600  |\n",
      "| │    │    │    └─(dropout)Dropout              | [2076, 600]                       | [2076, 600]    | --         |\n",
      "| │    │    │    └─(act)ReLU                     | [2076, 600]                       | [2076, 600]    | --         |\n",
      "| │    │    │    └─(convs)ModuleList             | --                                | --             | 1,807,200  |\n",
      "| │    │    │    │    └─(0)GATv2Conv             | [2076, 300], [2, 4318]            | [2076, 600]    | 362,400    |\n",
      "| │    │    │    │    └─(1)GATv2Conv             | [2076, 600], [2, 4318]            | [2076, 600]    | 722,400    |\n",
      "| │    │    │    │    └─(2)GATv2Conv             | [2076, 600], [2, 4318]            | [2076, 600]    | 722,400    |\n",
      "| │    │    │    └─(norms)ModuleList             | --                                | --             | 2,400      |\n",
      "| │    │    │    │    └─(0)BatchNorm             | [2076, 600]                       | [2076, 600]    | 1,200      |\n",
      "| │    │    │    │    │    └─(module)BatchNorm1d | [2076, 600]                       | [2076, 600]    | 1,200      |\n",
      "| │    │    │    │    └─(1)BatchNorm             | [2076, 600]                       | [2076, 600]    | 1,200      |\n",
      "| │    │    │    │    │    └─(module)BatchNorm1d | [2076, 600]                       | [2076, 600]    | 1,200      |\n",
      "| │    │    │    │    └─(2)Identity              | --                                | --             | --         |\n",
      "| │    │    └─(linear_layers)ModuleList          | --                                | --             | 270,600    |\n",
      "| │    │    │    └─(0)Linear                     | [2076, 600]                       | [2076, 300]    | 180,300    |\n",
      "| │    │    │    └─(1)Linear                     | [2076, 300]                       | [2076, 300]    | 90,300     |\n",
      "| │    │    └─(activation)ReLU                   | [2076, 300]                       | [2076, 300]    | --         |\n",
      "| │    └─(1)GATEncoder                           | [960, 300], [960, 960], [960]     | [960, 300]     | 2,080,200  |\n",
      "| │    │    └─(gat)GAT                           | [960, 300], [2, 14400]            | [960, 600]     | 1,809,600  |\n",
      "| │    │    │    └─(dropout)Dropout              | [960, 600]                        | [960, 600]     | --         |\n",
      "| │    │    │    └─(act)ReLU                     | [960, 600]                        | [960, 600]     | --         |\n",
      "| │    │    │    └─(convs)ModuleList             | --                                | --             | 1,807,200  |\n",
      "| │    │    │    │    └─(0)GATv2Conv             | [960, 300], [2, 14400]            | [960, 600]     | 362,400    |\n",
      "| │    │    │    │    └─(1)GATv2Conv             | [960, 600], [2, 14400]            | [960, 600]     | 722,400    |\n",
      "| │    │    │    │    └─(2)GATv2Conv             | [960, 600], [2, 14400]            | [960, 600]     | 722,400    |\n",
      "| │    │    │    └─(norms)ModuleList             | --                                | --             | 2,400      |\n",
      "| │    │    │    │    └─(0)BatchNorm             | [960, 600]                        | [960, 600]     | 1,200      |\n",
      "| │    │    │    │    │    └─(module)BatchNorm1d | [960, 600]                        | [960, 600]     | 1,200      |\n",
      "| │    │    │    │    └─(1)BatchNorm             | [960, 600]                        | [960, 600]     | 1,200      |\n",
      "| │    │    │    │    │    └─(module)BatchNorm1d | [960, 600]                        | [960, 600]     | 1,200      |\n",
      "| │    │    │    │    └─(2)Identity              | --                                | --             | --         |\n",
      "| │    │    └─(linear_layers)ModuleList          | --                                | --             | 270,600    |\n",
      "| │    │    │    └─(0)Linear                     | [960, 600]                        | [960, 300]     | 180,300    |\n",
      "| │    │    │    └─(1)Linear                     | [960, 300]                        | [960, 300]     | 90,300     |\n",
      "| │    │    └─(activation)ReLU                   | [960, 300]                        | [960, 300]     | --         |\n",
      "| │    └─(2)GATEncoder                           | [320, 300], [320, 320], [320]     | [320, 300]     | 1,356,600  |\n",
      "| │    │    └─(gat)GAT                           | [320, 300], [2, 1600]             | [320, 600]     | 1,086,000  |\n",
      "| │    │    │    └─(dropout)Dropout              | [320, 600]                        | [320, 600]     | --         |\n",
      "| │    │    │    └─(act)ReLU                     | [320, 600]                        | [320, 600]     | --         |\n",
      "| │    │    │    └─(convs)ModuleList             | --                                | --             | 1,084,800  |\n",
      "| │    │    │    │    └─(0)GATv2Conv             | [320, 300], [2, 1600]             | [320, 600]     | 362,400    |\n",
      "| │    │    │    │    └─(1)GATv2Conv             | [320, 600], [2, 1600]             | [320, 600]     | 722,400    |\n",
      "| │    │    │    └─(norms)ModuleList             | --                                | --             | 1,200      |\n",
      "| │    │    │    │    └─(0)BatchNorm             | [320, 600]                        | [320, 600]     | 1,200      |\n",
      "| │    │    │    │    │    └─(module)BatchNorm1d | [320, 600]                        | [320, 600]     | 1,200      |\n",
      "| │    │    │    │    └─(1)Identity              | --                                | --             | --         |\n",
      "| │    │    └─(linear_layers)ModuleList          | --                                | --             | 270,600    |\n",
      "| │    │    │    └─(0)Linear                     | [320, 600]                        | [320, 300]     | 180,300    |\n",
      "| │    │    │    └─(1)Linear                     | [320, 300]                        | [320, 300]     | 90,300     |\n",
      "| │    │    └─(activation)ReLU                   | [320, 300]                        | [320, 300]     | --         |\n",
      "| ├─(linear)Sequential                           | [64, 300]                         | [64, 384]      | 685,384    |\n",
      "| │    └─(0)Linear                               | [64, 300]                         | [64, 1000]     | 301,000    |\n",
      "| │    └─(1)ReLU                                 | [64, 1000]                        | [64, 1000]     | --         |\n",
      "| │    └─(2)Linear                               | [64, 1000]                        | [64, 384]      | 384,384    |\n",
      "| │    └─(3)Dropout                              | [64, 384]                         | [64, 384]      | --         |\n",
      "+------------------------------------------------+-----------------------------------+----------------+------------+\n",
      "Loaded model from ./outputs/one_side_circle_256_full_32.pt, best validation score=0.5974075593534737, best validation loss=6.34839835533729\n",
      "------------EPOCH  33 ------------\n",
      "Batch 20  | 00:33:35 | Loss 4.9446\n",
      "Batch 40  | 00:33:46 | Loss 4.9345\n",
      "Batch 60  | 00:33:58 | Loss 4.9258\n",
      "Batch 80  | 00:34:09 | Loss 4.9198\n",
      "Batch 100 | 00:34:20 | Loss 4.9228\n",
      "Batch 120 | 00:34:32 | Loss 4.9154\n",
      "Batch 140 | 00:34:43 | Loss 4.9188\n",
      "Batch 160 | 00:34:55 | Loss 4.9155\n",
      "Batch 180 | 00:35:07 | Loss 4.9123\n",
      "Batch 200 | 00:35:18 | Loss 4.9114\n",
      "Batch 220 | 00:35:30 | Loss 4.9100\n",
      "Batch 240 | 00:35:42 | Loss 4.9107\n",
      "Batch 260 | 00:35:54 | Loss 4.9078\n",
      "Batch 280 | 00:36:05 | Loss 4.9083\n",
      "Batch 300 | 00:36:17 | Loss 4.9073\n",
      "Batch 320 | 00:36:29 | Loss 4.9078\n",
      "Batch 340 | 00:36:41 | Loss 4.9092\n",
      "Batch 360 | 00:36:53 | Loss 4.9068\n",
      "Batch 380 | 00:37:05 | Loss 4.9071\n",
      "Batch 400 | 00:37:17 | Loss 4.9045\n",
      "[LOSS] 4.9031 | [SCORE] 0.5917\n",
      "[LR] 1.00E-06\n",
      "------------EPOCH  34 ------------\n",
      "Batch 20  | 00:37:53 | Loss 4.8913\n",
      "Batch 40  | 00:38:05 | Loss 4.9008\n",
      "Batch 60  | 00:38:17 | Loss 4.9012\n",
      "Batch 80  | 00:38:29 | Loss 4.8998\n",
      "Batch 100 | 00:38:41 | Loss 4.9031\n",
      "Batch 120 | 00:38:53 | Loss 4.8983\n",
      "Batch 140 | 00:39:05 | Loss 4.8989\n",
      "Batch 160 | 00:39:17 | Loss 4.8962\n",
      "Batch 180 | 00:39:29 | Loss 4.8948\n",
      "Batch 200 | 00:39:41 | Loss 4.8960\n",
      "Batch 220 | 00:39:53 | Loss 4.8976\n",
      "Batch 240 | 00:40:05 | Loss 4.8982\n",
      "Batch 260 | 00:40:17 | Loss 4.8977\n",
      "Batch 280 | 00:40:29 | Loss 4.8961\n",
      "Batch 300 | 00:40:41 | Loss 4.8931\n",
      "Batch 320 | 00:40:53 | Loss 4.8974\n",
      "Batch 340 | 00:41:05 | Loss 4.8954\n",
      "Batch 360 | 00:41:17 | Loss 4.8957\n",
      "Batch 380 | 00:41:29 | Loss 4.8922\n",
      "Batch 400 | 00:41:41 | Loss 4.8948\n",
      "[LOSS] 4.8931 | [SCORE] 0.5844\n",
      "[LR] 9.70E-07\n",
      "------------EPOCH  35 ------------\n",
      "Batch 20  | 00:42:16 | Loss 4.8784\n",
      "Batch 40  | 00:42:28 | Loss 4.8880\n",
      "Batch 60  | 00:42:40 | Loss 4.8904\n",
      "Batch 80  | 00:42:52 | Loss 4.8948\n",
      "Batch 100 | 00:43:04 | Loss 4.8917\n",
      "Batch 120 | 00:43:16 | Loss 4.8895\n",
      "Batch 140 | 00:43:27 | Loss 4.8947\n",
      "Batch 160 | 00:43:39 | Loss 4.8889\n",
      "Batch 180 | 00:43:51 | Loss 4.8904\n",
      "Batch 200 | 00:44:03 | Loss 4.8880\n",
      "Batch 220 | 00:44:15 | Loss 4.8932\n",
      "Batch 240 | 00:44:27 | Loss 4.8877\n",
      "Batch 260 | 00:44:39 | Loss 4.8905\n",
      "Batch 280 | 00:44:50 | Loss 4.8896\n",
      "Batch 300 | 00:45:02 | Loss 4.8888\n",
      "Batch 320 | 00:45:13 | Loss 4.8881\n",
      "Batch 340 | 00:45:25 | Loss 4.8906\n",
      "Batch 360 | 00:45:37 | Loss 4.8942\n",
      "Batch 380 | 00:45:48 | Loss 4.8891\n",
      "Batch 400 | 00:46:00 | Loss 4.8874\n",
      "[LOSS] 4.8872 | [SCORE] 0.5592\n",
      "[LR] 9.41E-07\n",
      "------------EPOCH  36 ------------\n",
      "Batch 20  | 00:46:36 | Loss 4.8759\n",
      "Batch 40  | 00:46:48 | Loss 4.8867\n",
      "Batch 60  | 00:47:00 | Loss 4.8870\n",
      "Batch 80  | 00:47:12 | Loss 4.8853\n",
      "Batch 100 | 00:47:24 | Loss 4.8850\n",
      "Batch 120 | 00:47:35 | Loss 4.8878\n",
      "Batch 140 | 00:47:47 | Loss 4.8852\n",
      "Batch 160 | 00:47:59 | Loss 4.8861\n",
      "Batch 180 | 00:48:10 | Loss 4.8852\n",
      "Batch 200 | 00:48:22 | Loss 4.8882\n",
      "Batch 220 | 00:48:34 | Loss 4.8852\n",
      "Batch 240 | 00:48:45 | Loss 4.8872\n",
      "Batch 260 | 00:48:57 | Loss 4.8869\n",
      "Batch 280 | 00:49:09 | Loss 4.8835\n",
      "Batch 300 | 00:49:20 | Loss 4.8863\n",
      "Batch 320 | 00:49:32 | Loss 4.8871\n",
      "Batch 340 | 00:49:44 | Loss 4.8825\n",
      "Batch 360 | 00:49:56 | Loss 4.8845\n",
      "Batch 380 | 00:50:08 | Loss 4.8838\n",
      "Batch 400 | 00:50:20 | Loss 4.8876\n",
      "[LOSS] 4.8834 | [SCORE] 0.5564\n",
      "[LR] 9.13E-07\n",
      "------------EPOCH  37 ------------\n",
      "Batch 20  | 00:50:54 | Loss 4.8727\n",
      "Batch 40  | 00:51:06 | Loss 4.8820\n",
      "Batch 60  | 00:51:18 | Loss 4.8844\n",
      "Batch 80  | 00:51:30 | Loss 4.8869\n",
      "Batch 100 | 00:51:42 | Loss 4.8839\n",
      "Batch 120 | 00:51:53 | Loss 4.8842\n",
      "Batch 140 | 00:52:05 | Loss 4.8796\n",
      "Batch 160 | 00:52:17 | Loss 4.8856\n",
      "Batch 180 | 00:52:29 | Loss 4.8827\n",
      "Batch 200 | 00:52:41 | Loss 4.8813\n",
      "Batch 220 | 00:52:53 | Loss 4.8824\n",
      "Batch 240 | 00:53:05 | Loss 4.8822\n",
      "Batch 260 | 00:53:16 | Loss 4.8816\n",
      "Batch 280 | 00:53:28 | Loss 4.8822\n",
      "Batch 300 | 00:53:40 | Loss 4.8818\n",
      "Batch 320 | 00:53:52 | Loss 4.8827\n",
      "Batch 340 | 00:54:04 | Loss 4.8810\n",
      "Batch 360 | 00:54:16 | Loss 4.8819\n",
      "Batch 380 | 00:54:28 | Loss 4.8810\n",
      "Batch 400 | 00:54:40 | Loss 4.8811\n",
      "[LOSS] 4.8798 | [SCORE] 0.5546\n",
      "[LR] 8.85E-07\n",
      "------------EPOCH  38 ------------\n",
      "Batch 20  | 00:55:14 | Loss 4.8655\n",
      "Batch 40  | 00:55:26 | Loss 4.8802\n",
      "Batch 60  | 00:55:38 | Loss 4.8803\n",
      "Batch 80  | 00:55:50 | Loss 4.8805\n",
      "Batch 100 | 00:56:01 | Loss 4.8811\n",
      "Batch 120 | 00:56:13 | Loss 4.8801\n",
      "Batch 140 | 00:56:25 | Loss 4.8808\n",
      "Batch 160 | 00:56:36 | Loss 4.8818\n",
      "Batch 180 | 00:56:49 | Loss 4.8778\n",
      "Batch 200 | 00:57:01 | Loss 4.8791\n",
      "Batch 220 | 00:57:13 | Loss 4.8787\n",
      "Batch 240 | 00:57:25 | Loss 4.8788\n",
      "Batch 260 | 00:57:37 | Loss 4.8790\n",
      "Batch 280 | 00:57:48 | Loss 4.8811\n",
      "Batch 300 | 00:58:00 | Loss 4.8785\n",
      "Batch 320 | 00:58:12 | Loss 4.8791\n",
      "Batch 340 | 00:58:25 | Loss 4.8769\n",
      "Batch 360 | 00:58:37 | Loss 4.8794\n",
      "Batch 380 | 00:58:48 | Loss 4.8793\n",
      "Batch 400 | 00:59:00 | Loss 4.8787\n",
      "[LOSS] 4.8772 | [SCORE] 0.5435\n",
      "[LR] 8.59E-07\n",
      "------------EPOCH  39 ------------\n",
      "Batch 20  | 00:59:36 | Loss 4.8655\n",
      "Batch 40  | 00:59:48 | Loss 4.8784\n",
      "Batch 60  | 01:00:00 | Loss 4.8782\n",
      "Batch 80  | 01:00:14 | Loss 4.8789\n",
      "Batch 100 | 01:00:26 | Loss 4.8743\n",
      "Batch 120 | 01:00:37 | Loss 4.8765\n",
      "Batch 140 | 01:00:49 | Loss 4.8806\n",
      "Batch 160 | 01:01:01 | Loss 4.8792\n",
      "Batch 180 | 01:01:12 | Loss 4.8747\n",
      "Batch 200 | 01:01:24 | Loss 4.8760\n",
      "Batch 220 | 01:01:35 | Loss 4.8746\n",
      "Batch 240 | 01:01:47 | Loss 4.8793\n",
      "Batch 260 | 01:01:59 | Loss 4.8777\n",
      "Batch 280 | 01:02:11 | Loss 4.8768\n",
      "Batch 300 | 01:02:23 | Loss 4.8830\n",
      "Batch 320 | 01:02:35 | Loss 4.8749\n",
      "Batch 340 | 01:02:47 | Loss 4.8755\n",
      "Batch 360 | 01:02:58 | Loss 4.8758\n",
      "Batch 380 | 01:03:10 | Loss 4.8764\n",
      "Batch 400 | 01:03:22 | Loss 4.8731\n",
      "[LOSS] 4.8753 | [SCORE] 0.5406\n",
      "[LR] 8.33E-07\n",
      "------------EPOCH  40 ------------\n",
      "Batch 20  | 01:03:57 | Loss 4.8613\n",
      "Batch 40  | 01:04:09 | Loss 4.8728\n",
      "Batch 60  | 01:04:21 | Loss 4.8762\n",
      "Batch 80  | 01:04:32 | Loss 4.8751\n",
      "Batch 100 | 01:04:44 | Loss 4.8748\n",
      "Batch 120 | 01:04:56 | Loss 4.8724\n",
      "Batch 140 | 01:05:08 | Loss 4.8730\n",
      "Batch 160 | 01:05:20 | Loss 4.8747\n",
      "Batch 180 | 01:05:32 | Loss 4.8746\n",
      "Batch 200 | 01:05:44 | Loss 4.8754\n",
      "Batch 220 | 01:05:56 | Loss 4.8726\n",
      "Batch 240 | 01:06:08 | Loss 4.8777\n",
      "Batch 260 | 01:06:19 | Loss 4.8748\n",
      "Batch 280 | 01:06:31 | Loss 4.8738\n",
      "Batch 300 | 01:06:43 | Loss 4.8751\n",
      "Batch 320 | 01:06:54 | Loss 4.8724\n",
      "Batch 340 | 01:07:06 | Loss 4.8747\n",
      "Batch 360 | 01:07:18 | Loss 4.8722\n",
      "Batch 380 | 01:07:29 | Loss 4.8750\n",
      "Batch 400 | 01:07:41 | Loss 4.8761\n",
      "[LOSS] 4.8734 | [SCORE] 0.5307\n",
      "[LR] 8.08E-07\n",
      "------------EPOCH  41 ------------\n",
      "Batch 20  | 01:08:16 | Loss 4.8590\n",
      "Batch 40  | 01:08:27 | Loss 4.8745\n",
      "Batch 60  | 01:08:39 | Loss 4.8747\n",
      "Batch 80  | 01:08:51 | Loss 4.8704\n",
      "Batch 100 | 01:09:03 | Loss 4.8734\n",
      "Batch 120 | 01:09:14 | Loss 4.8755\n",
      "Batch 140 | 01:09:26 | Loss 4.8743\n",
      "Batch 160 | 01:09:38 | Loss 4.8728\n",
      "Batch 180 | 01:09:50 | Loss 4.8730\n",
      "Batch 200 | 01:10:02 | Loss 4.8733\n",
      "Batch 220 | 01:10:14 | Loss 4.8716\n",
      "Batch 240 | 01:10:26 | Loss 4.8748\n",
      "Batch 260 | 01:10:37 | Loss 4.8731\n",
      "Batch 280 | 01:10:49 | Loss 4.8712\n",
      "Batch 300 | 01:11:01 | Loss 4.8724\n",
      "Batch 320 | 01:11:13 | Loss 4.8713\n",
      "Batch 340 | 01:11:25 | Loss 4.8748\n",
      "Batch 360 | 01:11:37 | Loss 4.8740\n",
      "Batch 380 | 01:11:49 | Loss 4.8737\n",
      "Batch 400 | 01:12:01 | Loss 4.8713\n",
      "[LOSS] 4.8718 | [SCORE] 0.5298\n",
      "[LR] 7.84E-07\n",
      "------------EPOCH  42 ------------\n",
      "Batch 20  | 01:12:35 | Loss 4.8580\n",
      "Batch 40  | 01:12:47 | Loss 4.8691\n",
      "Batch 60  | 01:12:59 | Loss 4.8696\n",
      "Batch 80  | 01:13:11 | Loss 4.8720\n",
      "Batch 100 | 01:13:23 | Loss 4.8723\n",
      "Batch 120 | 01:13:35 | Loss 4.8711\n",
      "Batch 140 | 01:13:47 | Loss 4.8704\n",
      "Batch 160 | 01:13:59 | Loss 4.8714\n",
      "Batch 180 | 01:14:11 | Loss 4.8695\n",
      "Batch 200 | 01:14:23 | Loss 4.8715\n",
      "Batch 220 | 01:14:35 | Loss 4.8686\n",
      "Batch 240 | 01:14:47 | Loss 4.8721\n",
      "Batch 260 | 01:15:00 | Loss 4.8743\n",
      "Batch 280 | 01:15:12 | Loss 4.8701\n",
      "Batch 300 | 01:15:23 | Loss 4.8697\n",
      "Batch 320 | 01:15:35 | Loss 4.8694\n",
      "Batch 340 | 01:15:47 | Loss 4.8738\n",
      "Batch 360 | 01:15:59 | Loss 4.8689\n",
      "Batch 380 | 01:16:11 | Loss 4.8731\n",
      "Batch 400 | 01:16:23 | Loss 4.8698\n",
      "[LOSS] 4.8703 | [SCORE] 0.5229\n",
      "[LR] 7.60E-07\n",
      "------------EPOCH  43 ------------\n",
      "Batch 20  | 01:16:59 | Loss 4.8565\n",
      "Batch 40  | 01:17:11 | Loss 4.8690\n",
      "Batch 60  | 01:17:23 | Loss 4.8691\n",
      "Batch 80  | 01:17:35 | Loss 4.8713\n",
      "Batch 100 | 01:17:47 | Loss 4.8678\n",
      "Batch 120 | 01:17:59 | Loss 4.8717\n",
      "Batch 140 | 01:18:11 | Loss 4.8715\n",
      "Batch 160 | 01:18:23 | Loss 4.8699\n",
      "Batch 180 | 01:18:35 | Loss 4.8686\n",
      "Batch 200 | 01:18:47 | Loss 4.8732\n",
      "Batch 220 | 01:18:59 | Loss 4.8666\n",
      "Batch 240 | 01:19:11 | Loss 4.8672\n",
      "Batch 260 | 01:19:24 | Loss 4.8655\n",
      "Batch 280 | 01:19:36 | Loss 4.8694\n",
      "Batch 300 | 01:19:48 | Loss 4.8700\n",
      "Batch 320 | 01:20:00 | Loss 4.8722\n",
      "Batch 340 | 01:20:12 | Loss 4.8678\n",
      "Batch 360 | 01:20:24 | Loss 4.8710\n",
      "Batch 380 | 01:20:36 | Loss 4.8694\n",
      "Batch 400 | 01:20:48 | Loss 4.8697\n",
      "[LOSS] 4.8685 | [SCORE] 0.5309\n",
      "[LR] 7.37E-07\n",
      "------------EPOCH  44 ------------\n",
      "Batch 20  | 01:21:23 | Loss 4.8561\n",
      "Batch 40  | 01:21:35 | Loss 4.8666\n",
      "Batch 60  | 01:21:47 | Loss 4.8676\n",
      "Batch 80  | 01:22:00 | Loss 4.8661\n",
      "Batch 100 | 01:22:12 | Loss 4.8673\n",
      "Batch 120 | 01:22:24 | Loss 4.8685\n",
      "Batch 140 | 01:22:36 | Loss 4.8678\n",
      "Batch 160 | 01:22:47 | Loss 4.8660\n",
      "Batch 180 | 01:22:59 | Loss 4.8715\n",
      "Batch 200 | 01:23:11 | Loss 4.8657\n",
      "Batch 220 | 01:23:23 | Loss 4.8719\n",
      "Batch 240 | 01:23:35 | Loss 4.8653\n",
      "Batch 260 | 01:23:47 | Loss 4.8674\n",
      "Batch 280 | 01:23:59 | Loss 4.8689\n",
      "Batch 300 | 01:24:11 | Loss 4.8643\n",
      "Batch 320 | 01:24:23 | Loss 4.8709\n",
      "Batch 340 | 01:24:35 | Loss 4.8686\n",
      "Batch 360 | 01:24:46 | Loss 4.8677\n",
      "Batch 380 | 01:24:58 | Loss 4.8681\n",
      "Batch 400 | 01:25:10 | Loss 4.8666\n",
      "[LOSS] 4.8669 | [SCORE] 0.5216\n",
      "[LR] 7.15E-07\n",
      "------------EPOCH  45 ------------\n",
      "Batch 20  | 01:25:45 | Loss 4.8524\n",
      "Batch 40  | 01:25:57 | Loss 4.8674\n",
      "Batch 60  | 01:26:09 | Loss 4.8660\n",
      "Batch 80  | 01:26:20 | Loss 4.8670\n",
      "Batch 100 | 01:26:32 | Loss 4.8685\n",
      "Batch 120 | 01:26:44 | Loss 4.8669\n",
      "Batch 140 | 01:26:56 | Loss 4.8652\n",
      "Batch 160 | 01:27:08 | Loss 4.8657\n",
      "Batch 180 | 01:27:20 | Loss 4.8688\n",
      "Batch 200 | 01:27:31 | Loss 4.8678\n",
      "Batch 220 | 01:27:43 | Loss 4.8648\n",
      "Batch 240 | 01:27:55 | Loss 4.8666\n",
      "Batch 260 | 01:28:07 | Loss 4.8657\n",
      "Batch 280 | 01:28:19 | Loss 4.8672\n",
      "Batch 300 | 01:28:30 | Loss 4.8655\n",
      "Batch 320 | 01:28:42 | Loss 4.8664\n",
      "Batch 340 | 01:28:54 | Loss 4.8715\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mMultiplicativeLR(\n\u001b[1;32m     14\u001b[0m     optimizer,\n\u001b[1;32m     15\u001b[0m     lr_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch: \u001b[38;5;241m0.97\u001b[39m,\n\u001b[1;32m     16\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m save_path, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcircle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mone_side_circle_256_full_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/02-COURS/3A/S1/altegrad/challenge/utils.py:114\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, nb_epochs, device, load_from, save_name, scheduler, metrics, initial_freeze, print_every, load_optimizer)\u001b[0m\n\u001b[1;32m    112\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m--> 114\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     loss_averager \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m batch_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/02-COURS/3A/S1/altegrad/challenge/utils.py:27\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(batch, model, optimizer, device, loss)\u001b[0m\n\u001b[1;32m     24\u001b[0m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m graph_batch \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 27\u001b[0m x_graph, x_text \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m loss(x_graph, x_text)\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/altegrad/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/altegrad/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/02-COURS/3A/S1/altegrad/challenge/models/diffpool/diffpool.py:238\u001b[0m, in \u001b[0;36mDiffPoolModel.forward\u001b[0;34m(self, graph_batch, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph_batch, input_ids, attention_mask):\n\u001b[0;32m--> 238\u001b[0m     graph_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     text_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder(input_ids, attention_mask)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph_encoded, text_encoded\n",
      "File \u001b[0;32m~/miniconda3/envs/altegrad/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/altegrad/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/02-COURS/3A/S1/altegrad/challenge/models/diffpool/diffpool.py:176\u001b[0m, in \u001b[0;36mDiffPoolEncoder.forward\u001b[0;34m(self, graph_batch)\u001b[0m\n\u001b[1;32m    170\u001b[0m S \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_layers[\u001b[38;5;241m0\u001b[39m](X, A, batch),\n\u001b[1;32m    172\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    173\u001b[0m )\n\u001b[1;32m    174\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_layers[\u001b[38;5;241m0\u001b[39m](X, A, batch)\n\u001b[0;32m--> 176\u001b[0m X, A \u001b[38;5;241m=\u001b[39m \u001b[43mankward_diffpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Update batch for next pooling layer. Assumes nodes are ordered by graph.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    180\u001b[0m     [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_sizes[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m    181\u001b[0m )\u001b[38;5;241m.\u001b[39mto(X\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Documents/02-COURS/3A/S1/altegrad/challenge/models/diffpool/diffpool.py:79\u001b[0m, in \u001b[0;36mankward_diffpool\u001b[0;34m(S, Z, A, batch)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mankward_diffpool\u001b[39m(S, Z, A, batch):\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Computes diffpool operations for batched S, Z and A. Using this function supposes that all graphs in the batch have different numbers of nodes and have been batched together. This is the case for the first diffpool layer.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m        Matrix A of shape (num_nodes x num_nodes)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     segment_indices \u001b[38;5;241m=\u001b[39m \u001b[43mget_segment_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     X_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(segment_indices), S\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mS\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     81\u001b[0m     A_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(segment_indices), S\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], S\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mS\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Documents/02-COURS/3A/S1/altegrad/challenge/models/diffpool/diffpool.py:20\u001b[0m, in \u001b[0;36mget_segment_indices\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m end, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;241m!=\u001b[39m x:\n\u001b[1;32m     21\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend((start, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     22\u001b[0m         x \u001b[38;5;241m=\u001b[39m y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = DiffPoolModel(\n",
    "    model_name=model_name,\n",
    "    num_node_features=300,\n",
    "    nout=embeddings_dim,\n",
    ").to(device)\n",
    "\n",
    "train_loader, val_loader = load_dataset(tokenizer, text_encoder, device, batch_size=64)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=1e-6, betas=(0.9, 0.999), weight_decay=0.01\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiplicativeLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda epoch: 0.97,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "save_path, _, _ = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    scheduler=scheduler,\n",
    "    nb_epochs=300,\n",
    "    device=device,\n",
    "    print_every=20,\n",
    "    load_optimizer=True,\n",
    "    metrics=Metrics(loss=\"circle\"),\n",
    "    load_from=save_path,\n",
    "    save_name=\"one_side_circle_256_full_\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
